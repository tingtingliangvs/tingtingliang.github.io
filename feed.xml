<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://tingtingliangvs.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tingtingliangvs.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-10-08T06:15:56+00:00</updated><id>https://tingtingliangvs.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework</title><link href="https://tingtingliangvs.github.io/blog/2022/bevfusion/" rel="alternate" type="text/html" title="BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework"/><published>2022-12-29T00:00:00+00:00</published><updated>2022-12-29T00:00:00+00:00</updated><id>https://tingtingliangvs.github.io/blog/2022/bevfusion</id><content type="html" xml:base="https://tingtingliangvs.github.io/blog/2022/bevfusion/"><![CDATA[<h2 id="why-multi-sensor-for-perception">Why multi-sensor for perception?</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/bevfusion/lidarmiss.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An example of a black car resulting in lower Lidar reflectance, causing LiDAR-only 3D detection difficulties. </div> <p>LiDAR and camera technology are essential for enabling self-driving cars to navigate their surroundings. However, these systems have their own limitations. Cameras provide detailed visual information, but can be affected by weather and do not offer reliable 3D data. LiDAR, on the other hand, provides 3D information but can struggle to accurately measure objects that are far away or are dark in color, as they absorb NIR （Near Infrared) radiation. As a result, relying solely on either LiDAR or camera data can lead to failures in complex scenarios.</p> <h2 id="how-to-fuse-information-from-multi-sensors">How to fuse information from multi-sensors?</h2> <p>In the early stage of perception systems, people design separate deep models for each sensor and fuse information via post-processing approaches. This method is limited by the loss of intermediate information. Recently, people have designed LiDAR-camera fusion deep networks to better leverage information from both modalities. Specifically, the majority of works can be summarized as follow: i) given one or a few points of the LiDAR point cloud, LiDAR to world transformation matrix and the essential matrix (camera to world); ii) people transform the LiDAR points or proposals into camera world and use them as queries, to select corresponding image features. This line of work constitutes the state-of-the-art methods of 3D perception.</p> <p>However, one underlying assumption that people overlooked is, that as one needs to generate image queries from LiDAR points, the current LiDAR-camera fusion methods intrinsically depend on the raw point cloud of the LiDAR sensor. In the realistic world, people discover that if the LiDAR sensor input is missing, for example, LiDAR points reflection rate is low due to object texture, a system glitch of internal data transfer, or even the field of view of the LiDAR sensor cannot reach 360 degrees due to hardware limitations, and current fusion methods fail to produce meaningful results. This fundamentally hinders the applicability of this line of work in the realistic autonomous driving system.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/bevfusion/prefusionvis.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Current fusion approaches often rely heavily on LiDAR input, making them prone to failure when LiDAR data is unavailable. </div> <p>We argue the ideal framework for LiDAR-camera fusion should be, that each model for a single modality should not fail regardless of the existence of the other modality, yet having both modalities will further boost the perception accuracy.</p> <h2 id="our-method">Our method</h2> <p>To this end, we propose a surprisingly simple yet effective framework that disentangles the LiDAR-camera fusion dependency of the current methods, dubbed BEVFusion. Specifically, our framework has two independent streams that encode the raw inputs from the camera and LiDAR sensors into features within the same BEV space (Why BEV? People have discovered that bird’s eye view (BEV) has been an de-facto standard for autonomous driving scenarios as, generally speaking, car cannot fly). We then design a simple module to fuse these BEV-level features after these two streams, so that the final feature can be passed into modern task prediction head architecture.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/bevfusion/bevfusionvis.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Our method is able to detect objects even when LiDAR data is missing. </div> <p>As our framework is a general approach, we can incorporate current single modality BEV models for camera and LiDAR into our framework. We moderately adopt Lift-Splat-Shoot as our camera stream, which projects multi-view image features to the 3D ego-car coordinate features to generate the camera BEV feature. Similarly, for the LiDAR stream, we select three popular models, two voxel-based ones and a pillar-based one to encode the LiDAR feature into the BEV space.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/publication_preview/bevfusion.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An overview of BEVFusion framework. </div> <h2 id="results">Results</h2> <p>On the nuScenes dataset, our simple framework shows great generalization ability. Following the same training settings, BEVFusion improves PointPillars and CenterPoint by 18.4% and 7.1% in mean average precision (mAP) respectively, and achieves a superior performance of 69.2% mAP comparing to 68.9% mAP of TransFusion, which is considered as state-of-the-art. Under the robust setting by randomly dropping the LiDAR points inside object bounding boxes with a probability of 0.5, we propose a novel augmentation technique and show that our framework surpasses all baselines significantly by a margin of 15.7%~28.9% mAP and demonstrate the robustness of our approach.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/bevfusion/bevvis.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> (a) We visualize the point clouds under the BEV perspective of two settings, limited field-of-view (FOV) and LiDAR fails to receive object reflection points, where the orange box indicates the object points are dropped. Blue boxes are bounding boxes and red-circled boxes are false-positive predictions. (b) We show the predictions of the state-of-the-art method, TransFusion, and ours under three settings. Obviously, the current fusion approaches fail inevitably when the LiDAR input is missing, while our framework can leverage the camera stream to recover these objects. </div> <h2 id="overview-video">Overview Video</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a href="https://neurips.cc/virtual/2022/poster/55002" title="YouTube"><figure> <picture> <img src="/assets/img/bevfusion/video.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </a> </div> </div> <div class="caption"> Click to watch video. </div> <h2 id="concurrent-works">Concurrent works</h2> <p>Some concurrent works also focus on fusing LiDAR-camera in 3D space:</p> <p>BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird’s-Eye View Representation. <a href="https://bevfusion.mit.edu/">MIT HAN Lab</a></p> <p>FUTR3D: A Unified Sensor Fusion Framework for 3D Detection. <a href="https://tsinghua-mars-lab.github.io/futr3d/">Tsinghua MARS Lab</a></p> <h2 id="further-information">Further Information</h2> <p>To learn more about work, watch our <a href="https://neurips.cc/virtual/2022/poster/55002">video in NeurIPS2022</a>.</p> <p>For more detailed information, check out our <a href="https://arxiv.org/abs/2205.13790">paper</a> and <a href="https://github.com/ADLab-AutoDrive/BEVFusion">code</a>. We are happy to receive your feedback!</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{liang2022bevfusion,
  title = {BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework},
  author = {Liang, Tingting and Xie, Hongwei and Yu, Kaicheng and Xia, Zhongyu and Lin, Zhiwei and Wang, Yongtao and Tang, Tao and Wang, Bing and Tang, Zhi},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year = {2022},
}
</code></pre></div></div>]]></content><author><name>Tingting Liang</name></author><summary type="html"><![CDATA[NeurIPS 2022]]></summary></entry><entry><title type="html">CBNet: A Composite Backbone Network Architecture for Object Detection</title><link href="https://tingtingliangvs.github.io/blog/2022/cbnetv2/" rel="alternate" type="text/html" title="CBNet: A Composite Backbone Network Architecture for Object Detection"/><published>2022-12-28T00:00:00+00:00</published><updated>2022-12-28T00:00:00+00:00</updated><id>https://tingtingliangvs.github.io/blog/2022/cbnetv2</id><content type="html" xml:base="https://tingtingliangvs.github.io/blog/2022/cbnetv2/"><![CDATA[<h2 id="abstract">Abstract</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/cbnet/pipeline.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Illustration of the proposed Composite Backbone Network (CBNet) architecture for object detection. </div> <p>Modern top-performing object detectors depend heavily on backbone networks, whose advances bring consistent performance gains through exploring more effective network structures. In this paper, we propose a novel and flexible backbone framework, namely , to construct high-performance detectors using existing open-source pre-trained backbones under the pre-training fine-tuning paradigm. In particular, CBNet architecture groups multiple identical backbones, which are connected through composite connections. Specifically, it integrates the high- and low-level features of multiple identical backbone networks and gradually expands the receptive field to more effectively perform object detection. We also propose a better training strategy with auxiliary supervision for CBNet-based detectors. CBNet has strong generalization capabilities for different backbones and head designs of the detector architecture. Without additional pre-training of the composite backbone, CBNet can be adapted to various backbones (i.e., CNN-based vs. Transformer-based) and head designs of most mainstream detectors (i.e., one-stage vs. two-stage, anchor-based vs. anchor-free-based). Experiments provide strong evidence that, compared with simply increasing the depth and width of the network, CBNet introduces a more efficient, effective, and resource-friendly way to build high-performance backbone networks. Particularly, our CB-Swin-L achieves 59.4% box AP and 51.6% mask AP on COCO test-dev under the single-model and single-scale testing protocol, which are significantly better than the state-of-the-art results (i.e., 57.7% box AP and 50.2% mask AP) achieved by Swin-L, while reducing the training time by 6x. With multi-scale testing, we push the current best single model result to a new record of 60.1% box AP and 52.3% mask AP without using extra training data.</p> <h2 id="results">Results</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/cbnet/compare.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Performance comparison of CBNet with different numbers of composite backbones (K) and pruning strategies. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/cbnet/coco.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Comparison with the state-of-the-art object detection and instance segmentation results on COCO. In collaboration with Swin Transformer, our CBNet achieves the state-of-the-art box AP and mask AP while using fewer training epochs. </div> <h2 id="further-information">Further Information</h2> <p>For more detailed information, check out our <a href="https://arxiv.org/abs/2107.00420">paper</a> and <a href="https://github.com/VDIGPKU/CBNetV2">code</a>. We are happy to receive your feedback!</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{DBLP:journals/tip/LiangCLWTCCL22,
  author    = {Ting{-}Ting Liang and
               Xiaojie Chu and
               Yudong Liu and
               Yongtao Wang and
               Zhi Tang and
               Wei Chu and
               Jingdong Chen and
               Haibin Ling},
  title     = {CBNet: {A} Composite Backbone Network Architecture for Object Detection},
  journal   = {IEEE Trans. Image Process.},
  volume    = {31},
  pages     = {6893--6906},
  year      = {2022},
  url       = {https://doi.org/10.1109/TIP.2022.3216771},
  doi       = {10.1109/TIP.2022.3216771},
  timestamp = {Mon, 05 Dec 2022 13:33:25 +0100},
  biburl    = {https://dblp.org/rec/journals/tip/LiangCLWTCCL22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}
</code></pre></div></div>]]></content><author><name>Tingting Liang</name></author><summary type="html"><![CDATA[TIP]]></summary></entry><entry><title type="html">OPANAS: One-Shot Path Aggregation Network Architecture Search for Object Detection</title><link href="https://tingtingliangvs.github.io/blog/2022/opanas/" rel="alternate" type="text/html" title="OPANAS: One-Shot Path Aggregation Network Architecture Search for Object Detection"/><published>2022-12-27T00:00:00+00:00</published><updated>2022-12-27T00:00:00+00:00</updated><id>https://tingtingliangvs.github.io/blog/2022/opanas</id><content type="html" xml:base="https://tingtingliangvs.github.io/blog/2022/opanas/"><![CDATA[<h2 id="abstract">Abstract</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opanas/pipeline.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 1. Single-path FPN super-net from SPOS search space. 2. Our OPANAS: (a) super-net training, i.e., the optimization of super-net weights; (b) optimal sub-net search with an evolutionary algorithm; (c) the searched optimal architecture. </div> <p>Recently, neural architecture search (NAS) has been exploited to design feature pyramid networks (FPNs) and achieved promising results for visual object detection. Encouraged by the success, we propose a novel One-Shot Path Aggregation Network Architecture Search (OPANAS) algorithm, which significantly improves both searching efficiency and detection accuracy. Specifically, we first introduce six heterogeneous information paths to build our search space, namely top-down, bottom-up, fusing-splitting, scale-equalizing, skip-connect and none. Second, we propose a novel search space of FPNs, in which each FPN candidate is represented by a densely-connected directed acyclic graph (each node is a feature pyramid and each edge is one of the six heterogeneous information paths). Third, we propose an efficient one-shot search method to find the optimal path aggregation architecture, that is, we first train a super-net and then find the optimal candidate with an evolutionary algorithm. Experimental results demonstrate the efficacy of the proposed OPANAS for object detection: (1) OPANAS is more efficient than state-of-the-art methods (i.e., NAS-FPN and Auto-FPN), at significantly smaller searching cost (i.e., only 4 GPU days on MS-COCO); (2) the optimal architecture found by OPANAS significantly improves main-stream detectors including RetinaNet, Faster R-CNN and Cascade R-CNN, by 2.3~3.2% mAP comparing to their FPN counterparts; and (3) a new state-of-the-art accuracy-speed trade-off (52.2% mAP at 7.6 FPS) at smaller training costs than comparable state-of-the-arts.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/opanas/infopaths.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The proposed six heterogeneous information paths mapping 4-level pyramid features $\{P_2, P_3, P_4, P_5\}$ to $\{F_2, F_3, F_4, F_5\}$. (a)-(d) are parameterized and (e)-(f) are parameter-free. </div> <h2 id="overview-video">Overview Video</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a href="https://www.youtube.com/watch?v=Erdso3frbEA" title="YouTube"><figure> <picture> <img src="/assets/img/opanas/youtube.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </a> </div> </div> <div class="caption"> Click to watch video. </div> <h2 id="further-information">Further Information</h2> <p>For more detailed information, check out our <a href="https://arxiv.org/abs/2103.04507">paper</a> and <a href="https://github.com/VDIGPKU/OPANAS">code</a>. We are happy to receive your feedback!</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{DBLP:conf/cvpr/LiangWTHL21,
  author    = {Tingting Liang and
               Yongtao Wang and
               Zhi Tang and
               Guosheng Hu and
               Haibin Ling},
  title     = {OPANAS: One-Shot Path Aggregation Network Architecture Search for
               Object Detection},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition, {CVPR}
               2021, virtual, June 19-25, 2021},
  pages     = {10195--10203},
  publisher = {Computer Vision Foundation / {IEEE}},
  year      = {2021},
}
</code></pre></div></div>]]></content><author><name>Tingting Liang</name></author><summary type="html"><![CDATA[CVPR 2021]]></summary></entry></feed>